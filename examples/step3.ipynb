{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensortrade.env.generic import Renderer\n",
    "\n",
    "\n",
    "class PositionChangeChart(Renderer):\n",
    "\n",
    "    def __init__(self, color: str = \"orange\"):\n",
    "        self.color = \"orange\"\n",
    "\n",
    "    def render(self, env, **kwargs):\n",
    "        history = pd.DataFrame(env.observer.renderer_history)\n",
    "\n",
    "        actions = list(history.action)\n",
    "        p = list(history.price)\n",
    "\n",
    "        buy = {}\n",
    "        sell = {}\n",
    "\n",
    "        for i in range(len(actions) - 1):\n",
    "            a1 = actions[i]\n",
    "            a2 = actions[i + 1]\n",
    "\n",
    "            if a1 != a2:\n",
    "                if a1 == 0 and a2 == 1:\n",
    "                    buy[i] = p[i]\n",
    "                else:\n",
    "                    sell[i] = p[i]\n",
    "\n",
    "        buy = pd.Series(buy)\n",
    "        sell = pd.Series(sell)\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        fig.suptitle(\"Performance\")\n",
    "\n",
    "        axs[0].plot(np.arange(len(p)), p, label=\"price\", color=self.color)\n",
    "        axs[0].scatter(buy.index, buy.values, marker=\"^\", color=\"green\")\n",
    "        axs[0].scatter(sell.index, sell.values, marker=\"^\", color=\"red\")\n",
    "        axs[0].set_title(\"Trading Chart\")\n",
    "\n",
    "        performance_df = pd.DataFrame().from_dict(env.action_scheme.portfolio.performance, orient='index')\n",
    "        performance_df.plot(ax=axs[1])\n",
    "        axs[1].set_title(\"Net Worth\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "import tensortrade.env.default as default\n",
    "\n",
    "from tensortrade.feed.core import DataFeed, Stream\n",
    "from tensortrade.oms.exchanges import Exchange\n",
    "from tensortrade.oms.services.execution.simulated import execute_order\n",
    "from tensortrade.oms.wallets import Wallet, Portfolio\n",
    "from tensortrade.oms.instruments import Instrument\n",
    "from tensortrade.env.default.rewards import (\n",
    "    TensorTradeRewardScheme,\n",
    "    SimpleProfit,\n",
    "    RiskAdjustedReturns,\n",
    "    PBR,\n",
    ")\n",
    "from tensortrade.env.default.actions import (\n",
    "    BSH\n",
    ")\n",
    "\n",
    "USD = Instrument(\"USD\", 2, \"U.S. Dollar\")\n",
    "TTC = Instrument(\"TTC\", 8, \"TensorTrade Coin\")\n",
    "\n",
    "\n",
    "def create_env(config):\n",
    "    x = np.arange(0, 2*np.pi, 2*np.pi / 1001)\n",
    "    y = 50*np.sin(3*x) + 100\n",
    "\n",
    "    x = np.arange(0, 2*np.pi, 2*np.pi / 1000)\n",
    "    p = Stream.source(y, dtype=\"float\").rename(\"USD-TTC\")\n",
    "\n",
    "    bitfinex = Exchange(\"bitfinex\", service=execute_order)(\n",
    "        p\n",
    "    )\n",
    "\n",
    "    cash = Wallet(bitfinex, 100000 * USD)\n",
    "    asset = Wallet(bitfinex, 0 * TTC)\n",
    "\n",
    "    portfolio = Portfolio(USD, [\n",
    "        cash,\n",
    "        asset\n",
    "    ])\n",
    "\n",
    "    feed = DataFeed([\n",
    "        p,\n",
    "        p.rolling(window=10).mean().rename(\"fast\"),\n",
    "        p.rolling(window=50).mean().rename(\"medium\"),\n",
    "        p.rolling(window=100).mean().rename(\"slow\"),\n",
    "        p.log().diff().fillna(0).rename(\"lr\")\n",
    "    ])\n",
    "\n",
    "    reward_scheme = PBR(price=p)\n",
    "\n",
    "    action_scheme = BSH(\n",
    "        cash=cash,\n",
    "        asset=asset\n",
    "    ).attach(reward_scheme)\n",
    "\n",
    "    renderer_feed = DataFeed([\n",
    "        Stream.source(y, dtype=\"float\").rename(\"price\"),\n",
    "        Stream.sensor(action_scheme, lambda s: s.action, dtype=\"float\").rename(\"action\")\n",
    "    ])\n",
    "\n",
    "    environment = default.create(\n",
    "        feed=feed,\n",
    "        portfolio=portfolio,\n",
    "        action_scheme=action_scheme,\n",
    "        reward_scheme=reward_scheme,\n",
    "        renderer_feed=renderer_feed,\n",
    "        renderer=PositionChangeChart(),\n",
    "        window_size=config[\"window_size\"],\n",
    "        max_allowed_loss=0.6\n",
    "    )\n",
    "    return environment\n",
    "\n",
    "register_env(\"TradingEnv\", create_env)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 18:23:24,898\tINFO worker.py:1544 -- Started a local Ray instance. View the dashboard at \u001B[1m\u001B[32m127.0.0.1:8265 \u001B[39m\u001B[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:23:34,467\tWARNING algorithm_config.py:596 -- Cannot create PPOConfig from given `config_dict`! Property __stdout_file__ not supported.\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m 2023-03-16 18:23:41,460\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m 2023-03-16 18:23:41,460\tWARNING env.py:166 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m 2023-03-16 18:23:41,460\tDEBUG rollout_worker.py:1948 -- Creating policy for default_policy\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m 2023-03-16 18:23:41,460\tDEBUG catalog.py:781 -- Created preprocessor <ray.rllib.models.preprocessors.NoPreprocessor object at 0x000002A572529FA0>: Box(-inf, inf, (25, 5), float32) -> (25, 5)\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m 2023-03-16 18:23:41,470\tINFO policy.py:1214 -- Policy (worker=1) running on CPU.\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m 2023-03-16 18:23:41,470\tINFO torch_policy_v2.py:110 -- Found 0 visible cuda devices.\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m 2023-03-16 18:23:41,480\tINFO util.py:122 -- Using connectors:\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m 2023-03-16 18:23:41,480\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m         ClipRewardAgentConnector\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m         ObsPreprocessorConnector\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m         MeanStdObservationFilterAgentConnector\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m         StateBufferConnector\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m         ViewRequirementAgentConnector\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m 2023-03-16 18:23:41,480\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m         ConvertToNumpyConnector\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m         NormalizeActionsConnector\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m         ImmutableActionsConnector\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m 2023-03-16 18:23:41,480\tDEBUG rollout_worker.py:855 -- Created rollout worker with env <ray.rllib.env.vector_env.VectorEnvWrapper object at 0x000002A572529FA0> (<TradingEnv instance>), policies <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:23:41,490\tINFO worker_set.py:310 -- Inferred observation/action spaces from remote worker (local worker has no env): {'default_policy': (Box(-inf, inf, (25, 5), float32), Discrete(2)), '__env__': (Box(-inf, inf, (25, 5), float32), Discrete(2))}\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:23:41,490\tDEBUG rollout_worker.py:1948 -- Creating policy for default_policy\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:23:41,490\tDEBUG catalog.py:781 -- Created preprocessor <ray.rllib.models.preprocessors.NoPreprocessor object at 0x0000024E89277370>: Box(-inf, inf, (25, 5), float32) -> (25, 5)\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:23:41,500\tINFO policy.py:1214 -- Policy (worker=local) running on CPU.\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:23:41,500\tINFO torch_policy_v2.py:110 -- Found 0 visible cuda devices.\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:23:41,510\tINFO util.py:122 -- Using connectors:\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:23:41,510\tINFO util.py:123 --     AgentConnectorPipeline\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m         ClipRewardAgentConnector\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m         ObsPreprocessorConnector\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m         MeanStdObservationFilterAgentConnector\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m         StateBufferConnector\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m         ViewRequirementAgentConnector\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:23:41,510\tINFO util.py:124 --     ActionConnectorPipeline\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m         ConvertToNumpyConnector\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m         NormalizeActionsConnector\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m         ImmutableActionsConnector\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:23:41,510\tINFO rollout_worker.py:2043 -- Built policy map: <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:23:41,510\tINFO rollout_worker.py:2044 -- Built preprocessor map: {'default_policy': None}\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:23:41,510\tINFO rollout_worker.py:760 -- Built filter map: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {'default_policy': <ray.rllib.utils.filter.MeanStdFilter object at 0x0000024E892D3820>})\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:23:41,510\tDEBUG rollout_worker.py:855 -- Created rollout worker with env None (None), policies <PolicyMap lru-caching-capacity=100 policy-IDs=['default_policy']>\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:23:41,516\tWARNING util.py:67 -- Install gputil for GPU system monitoring.\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m 2023-03-16 18:23:41,525\tINFO rollout_worker.py:908 -- Generating sample batch of size 4000\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m 2023-03-16 18:23:50,376\tINFO rollout_worker.py:949 -- Completed sample batch:\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m \n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m { 'count': 4000,\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m   'policy_batches': { 'default_policy': { 'action_dist_inputs': np.ndarray((4000, 2), dtype=float32, min=-0.011, max=0.011, mean=0.0),\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m                                           'action_logp': np.ndarray((4000,), dtype=float32, min=-0.701, max=-0.685, mean=-0.693),\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m                                           'actions': np.ndarray((4000,), dtype=int64, min=0.0, max=1.0, mean=0.497),\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m                                           'advantages': np.ndarray((4000,), dtype=float32, min=-1.009, max=1.015, mean=-0.022),\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m                                           'agent_index': np.ndarray((4000,), dtype=int32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m                                           'eps_id': np.ndarray((4000,), dtype=int64, min=3.364822380514301e+16, max=9.75862803069121e+17, mean=6.039317876649581e+17),\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m                                           'infos': np.ndarray((4000,), dtype=object, head={}),\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m                                           'new_obs': np.ndarray((4000, 25, 5), dtype=float32, min=-3.697, max=4.9, mean=-0.006),\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m                                           'obs': np.ndarray((4000, 25, 5), dtype=float32, min=-3.779, max=4.9, mean=-0.01),\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m                                           'rewards': np.ndarray((4000,), dtype=float32, min=-1.0, max=1.0, mean=-0.022),\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m                                           't': np.ndarray((4000,), dtype=int32, min=0.0, max=532.0, mean=251.904),\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m                                           'terminateds': np.ndarray((4000,), dtype=bool, min=0.0, max=1.0, mean=0.002),\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m                                           'truncateds': np.ndarray((4000,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m                                           'unroll_id': np.ndarray((4000,), dtype=int32, min=1.0, max=15.0, mean=7.712),\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m                                           'value_targets': np.ndarray((4000,), dtype=float32, min=-1.0, max=1.0, mean=-0.022),\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m                                           'vf_preds': np.ndarray((4000,), dtype=float32, min=-0.015, max=0.01, mean=0.001)}},\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m   'type': 'MultiAgentBatch'}\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10644)\u001B[0m \n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:23:50,406\tDEBUG train_ops.py:156 -- == sgd epochs for default_policy ==\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<div class=\"trialProgress\">\n  <h3>Trial Progress</h3>\n  <table>\n<thead>\n<tr><th>Trial name                </th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>connector_metrics                                                                                                                                                                                                                   </th><th>counters                                                                                                                        </th><th>custom_metrics  </th><th>date               </th><th>done  </th><th style=\"text-align: right;\">  episode_len_mean</th><th>episode_media  </th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_mean</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episodes_this_iter</th><th style=\"text-align: right;\">  episodes_total</th><th>experiment_id                   </th><th>hostname       </th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip  </th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_faulty_episodes</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_in_flight_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                </th><th style=\"text-align: right;\">  pid</th><th>policy_reward_max  </th><th>policy_reward_mean  </th><th>policy_reward_min  </th><th>sampler_perf                                                                                                                                                                                                    </th><th>sampler_results                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           </th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th>timers                                                                                                                                                                       </th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th style=\"text-align: right;\">  timesteps_total</th><th style=\"text-align: right;\">  training_iteration</th><th style=\"text-align: right;\">   trial_id</th><th style=\"text-align: right;\">  warmup_time</th></tr>\n</thead>\n<tbody>\n<tr><td>PPO_TradingEnv_97786_00000</td><td style=\"text-align: right;\">                   4000</td><td>{&#x27;ClipRewardAgentConnector_ms&#x27;: 0.0, &#x27;ObsPreprocessorConnector_ms&#x27;: 0.0, &#x27;MeanStdObservationFilterAgentConnector_ms&#x27;: 0.14282975878034318, &#x27;StateBufferConnector_ms&#x27;: 0.0, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.028623853410993303}</td><td>{&#x27;num_env_steps_sampled&#x27;: 4000, &#x27;num_env_steps_trained&#x27;: 4000, &#x27;num_agent_steps_sampled&#x27;: 4000, &#x27;num_agent_steps_trained&#x27;: 4000}</td><td>{}              </td><td>2023-03-16_18-23-55</td><td>False </td><td style=\"text-align: right;\">           518.286</td><td>{}             </td><td style=\"text-align: right;\">                  38</td><td style=\"text-align: right;\">             -10.4286</td><td style=\"text-align: right;\">                 -49</td><td style=\"text-align: right;\">                   7</td><td style=\"text-align: right;\">               7</td><td>f34957b411b7450bb141ce4fd8edcecd</td><td>DESKTOP-VQNVLBP</td><td>{&#x27;learner&#x27;: {&#x27;default_policy&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: 0.0, &#x27;grad_gnorm&#x27;: 1.9787943588469619, &#x27;cur_kl_coeff&#x27;: 0.20000000000000004, &#x27;cur_lr&#x27;: 0.10000000000000002, &#x27;total_loss&#x27;: 0.3050557771517384, &#x27;policy_loss&#x27;: -0.19665496241231198, &#x27;vf_loss&#x27;: 0.9976007962739596, &#x27;vf_explained_var&#x27;: 0.0003735168646740657, &#x27;kl&#x27;: 0.04701697227046456, &#x27;entropy&#x27;: 0.6493055167377636, &#x27;entropy_coeff&#x27;: 0.01}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: 128.0, &#x27;num_grad_updates_lifetime&#x27;: 465.5, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: 464.5}}, &#x27;num_env_steps_sampled&#x27;: 4000, &#x27;num_env_steps_trained&#x27;: 4000, &#x27;num_agent_steps_sampled&#x27;: 4000, &#x27;num_agent_steps_trained&#x27;: 4000}</td><td style=\"text-align: right;\">                         1</td><td>127.0.0.1</td><td style=\"text-align: right;\">                     4000</td><td style=\"text-align: right;\">                     4000</td><td style=\"text-align: right;\">                   4000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                   4000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                    0</td><td style=\"text-align: right;\">                    1</td><td style=\"text-align: right;\">                         0</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                         4000</td><td>{&#x27;cpu_util_percent&#x27;: 21.235, &#x27;ram_util_percent&#x27;: 53.620000000000005}</td><td style=\"text-align: right;\">22016</td><td>{}                 </td><td>{}                  </td><td>{}                 </td><td>{&#x27;mean_raw_obs_processing_ms&#x27;: 0.31153674841940143, &#x27;mean_inference_ms&#x27;: 1.1878294278549808, &#x27;mean_action_processing_ms&#x27;: 0.15516729242829674, &#x27;mean_env_wait_ms&#x27;: 0.547459291774194, &#x27;mean_env_render_ms&#x27;: 0.0}</td><td>{&#x27;episode_reward_max&#x27;: 38.0, &#x27;episode_reward_min&#x27;: -49.0, &#x27;episode_reward_mean&#x27;: -10.428571428571429, &#x27;episode_len_mean&#x27;: 518.2857142857143, &#x27;episode_media&#x27;: {}, &#x27;episodes_this_iter&#x27;: 7, &#x27;policy_reward_min&#x27;: {}, &#x27;policy_reward_max&#x27;: {}, &#x27;policy_reward_mean&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [38.0, -6.0, -49.0, 9.0, -26.0, -39.0, 0.0], &#x27;episode_lengths&#x27;: [533, 527, 506, 518, 511, 516, 517]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: 0.31153674841940143, &#x27;mean_inference_ms&#x27;: 1.1878294278549808, &#x27;mean_action_processing_ms&#x27;: 0.15516729242829674, &#x27;mean_env_wait_ms&#x27;: 0.547459291774194, &#x27;mean_env_render_ms&#x27;: 0.0}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ClipRewardAgentConnector_ms&#x27;: 0.0, &#x27;ObsPreprocessorConnector_ms&#x27;: 0.0, &#x27;MeanStdObservationFilterAgentConnector_ms&#x27;: 0.14282975878034318, &#x27;StateBufferConnector_ms&#x27;: 0.0, &#x27;ViewRequirementAgentConnector_ms&#x27;: 0.028623853410993303}}</td><td style=\"text-align: right;\">              14.119</td><td style=\"text-align: right;\">            14.119</td><td style=\"text-align: right;\">        14.119</td><td>{&#x27;training_iteration_time_ms&#x27;: 14108.964, &#x27;load_time_ms&#x27;: 0.0, &#x27;load_throughput&#x27;: 0.0, &#x27;learn_time_ms&#x27;: 5216.867, &#x27;learn_throughput&#x27;: 766.744, &#x27;synch_weights_time_ms&#x27;: 9.97}</td><td style=\"text-align: right;\"> 1678962235</td><td style=\"text-align: right;\">                        0</td><td style=\"text-align: right;\">             4000</td><td style=\"text-align: right;\">                   1</td><td style=\"text-align: right;\">97786_00000</td><td style=\"text-align: right;\">      6.36223</td></tr>\n</tbody>\n</table>\n</div>\n<style>\n.trialProgress {\n  display: flex;\n  flex-direction: column;\n  color: var(--jp-ui-font-color1);\n}\n.trialProgress h3 {\n  font-weight: bold;\n}\n.trialProgress td {\n  white-space: nowrap;\n}\n</style>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:23:55,633\tDEBUG filter_manager.py:34 -- Synchronizing filters ...\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:23:55,633\tDEBUG filter_manager.py:55 -- Updating remote filters ...\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:23:55,643\tDEBUG algorithm.py:2300 -- synchronized filters: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {'default_policy': <ray.rllib.utils.filter.MeanStdFilter object at 0x0000024E892D3820>})\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:24:04,291\tDEBUG train_ops.py:156 -- == sgd epochs for default_policy ==\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:24:09,593\tDEBUG filter_manager.py:34 -- Synchronizing filters ...\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:24:09,593\tDEBUG filter_manager.py:55 -- Updating remote filters ...\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:24:09,599\tDEBUG algorithm.py:2300 -- synchronized filters: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {'default_policy': <ray.rllib.utils.filter.MeanStdFilter object at 0x0000024E892D3820>})\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:24:17,954\tDEBUG train_ops.py:156 -- == sgd epochs for default_policy ==\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:24:23,420\tDEBUG filter_manager.py:34 -- Synchronizing filters ...\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:24:23,421\tDEBUG filter_manager.py:55 -- Updating remote filters ...\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:24:23,421\tDEBUG algorithm.py:2300 -- synchronized filters: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {'default_policy': <ray.rllib.utils.filter.MeanStdFilter object at 0x0000024E892D3820>})\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:24:31,358\tDEBUG train_ops.py:156 -- == sgd epochs for default_policy ==\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:24:36,767\tDEBUG filter_manager.py:34 -- Synchronizing filters ...\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:24:36,767\tDEBUG filter_manager.py:55 -- Updating remote filters ...\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:24:36,767\tDEBUG algorithm.py:2300 -- synchronized filters: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {'default_policy': <ray.rllib.utils.filter.MeanStdFilter object at 0x0000024E892D3820>})\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:24:44,633\tDEBUG train_ops.py:156 -- == sgd epochs for default_policy ==\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:24:50,007\tDEBUG filter_manager.py:34 -- Synchronizing filters ...\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:24:50,012\tDEBUG filter_manager.py:55 -- Updating remote filters ...\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:24:50,013\tDEBUG algorithm.py:2300 -- synchronized filters: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {'default_policy': <ray.rllib.utils.filter.MeanStdFilter object at 0x0000024E892D3820>})\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:24:57,719\tDEBUG train_ops.py:156 -- == sgd epochs for default_policy ==\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:25:03,108\tDEBUG filter_manager.py:34 -- Synchronizing filters ...\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:25:03,113\tDEBUG filter_manager.py:55 -- Updating remote filters ...\n",
      "\u001B[2m\u001B[36m(PPO pid=22016)\u001B[0m 2023-03-16 18:25:03,118\tDEBUG algorithm.py:2300 -- synchronized filters: defaultdict(<class 'ray.rllib.utils.filter.NoFilter'>, {'default_policy': <ray.rllib.utils.filter.MeanStdFilter object at 0x0000024E892D3820>})\n",
      "2023-03-16 18:25:03,864\tINFO tune.py:798 -- Total run time: 97.90 seconds (95.19 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\n",
    "      \"episode_reward_mean\": 500\n",
    "    },\n",
    "    config={\n",
    "        \"env\": \"TradingEnv\",\n",
    "        \"env_config\": {\n",
    "            \"window_size\": 25\n",
    "        },\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"framework\": \"torch\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    },\n",
    "    checkpoint_at_end=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
